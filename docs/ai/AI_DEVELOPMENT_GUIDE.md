# AI Development Guide for Claude Orchestrator

**Version:** v10.1.0
**Target Audience:** AI Assistants (Claude Code, GPT-4, etc.)
**Last Updated:** 2025-10-21

---

## üéØ Purpose

This guide helps AI assistants understand and work with the Claude Orchestrator codebase effectively. It provides:
- Quick project context
- Development workflows
- Code patterns and conventions
- Common tasks and how to accomplish them
- Troubleshooting guidance

---

## üìñ Project Overview

### What is Claude Orchestrator?

A **world-class Python framework** for parallel AI task execution using multiple Claude CLI instances.

**Key Capabilities:**
- ‚úÖ Parallel task execution across multiple AI workers
- ‚úÖ REST API service for AI-to-AI orchestration
- ‚úÖ Interactive mode with safety controls
- ‚úÖ Web-based real-time dashboard
- ‚úÖ Enterprise-grade observability and resilience
- ‚úÖ 100% test coverage on functional tests

### Current Version: v10.1.0

**Architecture:** Clean Architecture with DDD principles
**Quality Level:** A++ (World-class)
**Test Coverage:** 100% functional tests passing
**Type Safety:** mypy strict mode (core modules)

---

## üöÄ Quick Start for AI Assistants

### Understanding the Project Structure

```
parallel_ai_test_project/
‚îú‚îÄ‚îÄ orchestrator/           # Core orchestration engine
‚îÇ   ‚îú‚îÄ‚îÄ core/              # Domain logic (V9+)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py          # Custom exceptions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ observability.py       # Metrics & monitoring
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resilience.py          # Circuit breaker, retry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ structured_logging.py  # JSON logging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validated_config.py    # Config validation
‚îÇ   ‚îú‚îÄ‚îÄ api/               # REST API (FastAPI)
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration
‚îÇ   ‚îî‚îÄ‚îÄ interfaces.py      # Abstract interfaces
‚îú‚îÄ‚îÄ web_ui/                # Real-time dashboard
‚îú‚îÄ‚îÄ tests/                 # Comprehensive test suite
‚îú‚îÄ‚îÄ docs/                  # Documentation
‚îî‚îÄ‚îÄ examples/              # Usage examples
```

### Key Version History

- **v10.1** (Latest): Version unification + Python 3.13 + Security
- **v10.0**: Clean Architecture refactoring (-49,412 lines)
- **v9.0**: Enterprise features (observability, resilience)
- **v7.0**: REST API service
- **v6.0**: Initial refactoring

**IMPORTANT:** `RefactoredOrchestrator` was removed in v10. Old tests in `test_orchestrator.py` are skipped.

---

## üîß Development Workflow

### 1. Making Code Changes

#### Before Starting
```bash
# Check current branch and status
git status
git log -1 --oneline

# Run tests to establish baseline
python -m pytest tests/ -v --tb=short --no-cov
```

#### Development Process
1. **Read relevant files first** - Use Read tool to understand existing code
2. **Make focused changes** - Edit one concern at a time
3. **Run tests immediately** - Verify changes don't break functionality
4. **Check type safety** - Run mypy on modified files
5. **Commit incrementally** - Small, focused commits

#### Example Workflow
```bash
# 1. Read the file you want to modify
Read: orchestrator/core/observability.py

# 2. Make your changes using Edit tool
Edit: orchestrator/core/observability.py

# 3. Test immediately
python -m pytest tests/test_integration_v9.py::TestObservability -v

# 4. Check types
mypy orchestrator/core/observability.py --config-file mypy.ini

# 5. Commit
git add orchestrator/core/observability.py
git commit -m "feat: Add new metric to observability system"
```

### 2. Running Tests

#### Quick Test Commands
```bash
# All tests
python -m pytest tests/ -v --tb=short --no-cov

# Specific module
python -m pytest tests/test_api_integration.py -v

# Specific test
python -m pytest tests/test_exceptions.py::TestOrchestratorError::test_basic_error -v

# With coverage
python -m pytest tests/ --cov=orchestrator --cov-report=html
```

#### Test Organization
- `test_api_*.py` - API endpoint tests
- `test_integration_v9.py` - V9 integration tests (observability, resilience)
- `test_enhanced_interactive_*.py` - Interactive mode tests
- `test_exceptions.py` - Exception handling tests
- `test_orchestrator.py` - **SKIPPED** (v10 removed RefactoredOrchestrator)

#### Expected Test Results
```
‚úÖ 73/73 functional tests PASSED (100%)
‚è≠Ô∏è 15 SKIPPED (obsolete v9 tests)
‚ö†Ô∏è 6 ERRORS (Windows file lock in teardown - tests pass)
‚ùå 0 FAILED
```

### 3. Type Checking with mypy

```bash
# Check specific file
mypy orchestrator/core/exceptions.py --config-file mypy.ini

# Check entire module
mypy orchestrator/ --config-file mypy.ini

# Core modules have strictest checking
# See mypy.ini for configuration
```

**Key Type Patterns:**
```python
from typing import Optional, Dict, Any, List
from pathlib import Path

def process_task(
    task_id: str,
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """Process a task with optional config"""
    pass

# For __exit__ methods
import types
from typing import Type

def __exit__(
    self,
    exc_type: Optional[Type[BaseException]],
    exc_val: Optional[BaseException],
    exc_tb: Optional[types.TracebackType]
) -> None:
    pass
```

---

## üìù Code Patterns and Conventions

### 1. Exception Handling

**Always use custom exceptions from `orchestrator/core/exceptions.py`:**

```python
from orchestrator.core.exceptions import (
    TaskExecutionError,
    WorkerTimeoutError,
    ConfigurationError
)

# Good
raise TaskExecutionError(
    "Task failed to execute",
    task_name="data_processing",
    worker_id="worker_1",
    context={"retry_count": 3}
)

# Bad - don't use generic exceptions
raise Exception("Something went wrong")
```

**Exception Hierarchy:**
- `OrchestratorException` - Base for all exceptions
- `WorkerError` - Worker-related errors
- `TaskError` - Task execution errors
- `APIError` - API-related errors
- `SafetyError` - Safety check failures

### 2. Configuration

**Use `OrchestratorConfig` dataclass:**

```python
from orchestrator.config import OrchestratorConfig

# Create config
config = OrchestratorConfig(
    execution_mode="windows",  # or "wsl"
    max_workers=10,
    default_timeout=120
)

# Or from environment
config = OrchestratorConfig.from_env()

# Access properties
claude_cmd = config.get_claude_command(
    input_file="task.txt",
    output_file="result.txt"
)
```

### 3. Logging (Structured)

**Use `StructuredLogger` for all logging:**

```python
from orchestrator.core.structured_logging import (
    StructuredLogger,
    LogLevel,
    LogCategory
)

logger = StructuredLogger(
    name="my_component",
    category=LogCategory.SYSTEM,
    level=LogLevel.INFO
)

# Basic logging
logger.info("Processing task", task_id="task_1")

# With context
logger.error(
    "Task failed",
    task_id="task_1",
    error_type="timeout",
    duration=120.5
)

# Performance tracking
with logger.track_operation("database_query") as op:
    result = execute_query()
    op.labels = {"query_type": "select", "table": "users"}
```

### 4. API Models (Pydantic)

**Use Pydantic models for API:**

```python
from pydantic import BaseModel, Field
from datetime import datetime

class TaskRequest(BaseModel):
    """Request model"""
    task_description: str = Field(..., min_length=10)
    priority: int = Field(default=0, ge=0, le=10)

    class Config:
        json_schema_extra = {
            "example": {
                "task_description": "Process customer data",
                "priority": 5
            }
        }

# Serialize for JSON response
response = TaskRequest(task_description="...", priority=5)
return response.model_dump(mode='json')  # Important for datetime!
```

### 5. Observability

**Use metrics and monitoring:**

```python
from orchestrator.core.observability import (
    MetricsCollector,
    PerformanceMonitor
)

collector = MetricsCollector()
monitor = PerformanceMonitor(collector)

# Track operations
monitor.track_operation(
    operation_name="api_call",
    duration=0.125,
    success=True,
    labels={"endpoint": "/users", "method": "GET"}
)

# Get statistics
stats = monitor.get_operation_stats("api_call")
# {'count': 10, 'mean': 0.15, 'p95': 0.25, ...}
```

### 6. Resilience Patterns

**Use circuit breakers and retries:**

```python
from orchestrator.core.resilience import (
    CircuitBreaker,
    RetryStrategy
)

# Circuit breaker
breaker = CircuitBreaker(
    failure_threshold=5,
    recovery_timeout=60.0,
    half_open_max_calls=3
)

with breaker:
    result = call_external_service()

# Retry strategy
retry = RetryStrategy(
    max_attempts=3,
    base_delay=1.0,
    max_delay=10.0,
    exponential_base=2.0
)

result = retry.execute(lambda: risky_operation())
```

---

## üéØ Common Tasks

### Task 1: Add a New API Endpoint

1. **Add model** in `orchestrator/api/models.py`:
```python
class NewFeatureRequest(BaseModel):
    param: str = Field(..., description="Parameter description")
```

2. **Add endpoint** in `orchestrator/api/app.py`:
```python
@app.post("/api/v1/new-feature", response_model=SuccessResponse)
async def new_feature(request: NewFeatureRequest):
    """New feature endpoint"""
    # Implementation
    return SuccessResponse(message="Success", data=result)
```

3. **Add tests** in `tests/test_api_integration.py`:
```python
def test_new_feature(client):
    response = client.post(
        "/api/v1/new-feature",
        json={"param": "value"},
        headers={"X-API-Key": "test-key"}
    )
    assert response.status_code == 200
```

4. **Run tests**:
```bash
python -m pytest tests/test_api_integration.py::test_new_feature -v
```

### Task 2: Add a New Exception Type

1. **Add exception** in `orchestrator/core/exceptions.py`:
```python
class NewSpecificError(OrchestratorException):
    """Description of when this error occurs"""

    def __init__(
        self,
        message: str,
        specific_param: str,
        context: Optional[Dict[str, Any]] = None
    ):
        context = context or {}
        context.update({"specific_param": specific_param})
        super().__init__(message, context=context)
        self.specific_param = specific_param
```

2. **Add tests** in `tests/test_exceptions.py`:
```python
def test_new_specific_error():
    error = NewSpecificError(
        "Error occurred",
        specific_param="value"
    )
    assert error.specific_param == "value"
    assert "value" in str(error)
```

### Task 3: Add Observability Metrics

1. **Add metric** in component:
```python
from orchestrator.core.observability import MetricsCollector

collector = MetricsCollector()
collector.record("custom_metric", 42.0, labels={"type": "gauge"})
```

2. **Add monitoring** with PerformanceMonitor:
```python
monitor = PerformanceMonitor(collector)
monitor.track_operation("custom_op", duration, success, labels)
```

3. **Test metrics**:
```python
def test_custom_metrics():
    collector = MetricsCollector()
    collector.record("test_metric", 100.0)

    summary = collector.get_summary()
    assert "test_metric" in summary
    assert summary["test_metric"]["count"] == 1
```

### Task 4: Fix Type Errors

Common type error patterns and fixes:

**Error: `Optional` parameter with `None` default**
```python
# Bad
def func(param: str = None):
    pass

# Good
from typing import Optional
def func(param: Optional[str] = None):
    pass
```

**Error: Missing return type**
```python
# Bad
def process():
    return result

# Good
def process() -> Dict[str, Any]:
    return result

# For no return
def setup() -> None:
    pass
```

**Error: `__exit__` signature**
```python
# Bad
def __exit__(self, exc_type, exc_val, exc_tb):
    pass

# Good
import types
from typing import Optional, Type

def __exit__(
    self,
    exc_type: Optional[Type[BaseException]],
    exc_val: Optional[BaseException],
    exc_tb: Optional[types.TracebackType]
) -> None:
    pass
```

---

## üêõ Troubleshooting

### Common Issues

#### 1. JSON Serialization Error
```
TypeError: Object of type datetime is not JSON serializable
```

**Fix:** Use `model_dump(mode='json')` for Pydantic models:
```python
# Bad
return response.model_dump()

# Good
return response.model_dump(mode='json')
```

#### 2. Import Errors After Refactoring
```
ImportError: cannot import name 'RefactoredOrchestrator'
```

**Fix:** `RefactoredOrchestrator` was removed in v10. Use service layer:
```python
# Old (v9 and earlier)
from orchestrator.main import RefactoredOrchestrator

# New (v10+)
from orchestrator.core.task_analyzer_service import TaskAnalyzerService
```

#### 3. Test Failures After Code Changes

**Steps to diagnose:**
```bash
# 1. Run specific failing test with full traceback
python -m pytest tests/test_xyz.py::test_name -v --tb=short

# 2. Check if test expectations changed
# Read test file and understand what it's testing

# 3. Verify your changes didn't break contracts
# Check interfaces and API contracts

# 4. Update test if behavior change is intentional
# Or fix code if test correctly identifies regression
```

#### 4. Windows File Lock Errors in Tests
```
PermissionError: [WinError 32] ... another process is using it
```

**Note:** These occur in teardown only. Tests themselves pass. This is a known Windows limitation with temporary files. Not a failure.

#### 5. mypy Type Check Failures

**Common fixes:**
```bash
# 1. Check mypy.ini configuration
cat mypy.ini

# 2. Add missing type imports
from typing import Optional, Dict, Any, List

# 3. Add return type annotations
def func() -> ReturnType:

# 4. Use Optional for nullable parameters
def func(param: Optional[str] = None):
```

---

## üìö Key Files Reference

### Configuration & Setup
- `mypy.ini` - Type checking configuration
- `pytest.ini` - Test configuration
- `pyproject.toml` - Project metadata
- `requirements.txt` - Dependencies

### Core Modules
- `orchestrator/config.py` - Configuration management
- `orchestrator/interfaces.py` - Abstract interfaces
- `orchestrator/core/exceptions.py` - Exception types
- `orchestrator/core/structured_logging.py` - Logging system
- `orchestrator/core/observability.py` - Metrics & monitoring
- `orchestrator/core/resilience.py` - Circuit breaker, retry

### API
- `orchestrator/api/app.py` - FastAPI application
- `orchestrator/api/models.py` - Pydantic models
- `orchestrator/api/auth.py` - Authentication

### Tests
- `tests/test_api_integration.py` - API tests
- `tests/test_integration_v9.py` - V9 features (observability, resilience)
- `tests/test_exceptions.py` - Exception tests
- `tests/test_enhanced_interactive_worker_manager.py` - Interactive mode

### Documentation
- `README.md` - Project overview
- `ARCHITECTURE.md` - Architecture details (see this file)
- `CODEBASE_MAP.md` - Navigation guide (see this file)
- `AI_PROMPTS.md` - Effective prompts (see this file)

---

## üéØ Best Practices for AI Assistants

### 1. Always Read Before Editing
```
‚ùå Don't: Guess file contents and edit blindly
‚úÖ Do: Use Read tool to understand current code first
```

### 2. Make Incremental Changes
```
‚ùå Don't: Change 10 files at once
‚úÖ Do: Change 1-2 related files, test, then continue
```

### 3. Test Immediately
```
‚ùå Don't: Make multiple changes then test at end
‚úÖ Do: Test after each logical change
```

### 4. Follow Type Hints
```
‚ùå Don't: Ignore mypy errors
‚úÖ Do: Fix type errors as you go
```

### 5. Use Existing Patterns
```
‚ùå Don't: Invent new patterns
‚úÖ Do: Follow established patterns in codebase
```

### 6. Preserve Code Quality
```
‚ùå Don't: Add quick hacks
‚úÖ Do: Maintain world-class quality standards
```

### 7. Document Context
```
‚ùå Don't: Make changes without explaining
‚úÖ Do: Add clear comments and commit messages
```

---

## üîÑ Version Compatibility

### Current Version: v10.1.0

**Breaking Changes from v9:**
- ‚ùå `RefactoredOrchestrator` removed
- ‚ùå Old orchestrator/main.py patterns obsolete
- ‚úÖ Use service layer (orchestrator/core/*)
- ‚úÖ Clean Architecture with DDD

**Migration Guide:**
```python
# v9 and earlier
from orchestrator.main import RefactoredOrchestrator
orch = RefactoredOrchestrator(config)
result = orch.execute(request)

# v10+
from orchestrator.core.task_analyzer_service import TaskAnalyzerService
service = TaskAnalyzerService(config)
result = service.analyze_and_execute(request)
```

---

## üÜò Getting Help

### For AI Assistants

1. **Check this guide first** - Most common tasks are covered
2. **Read related code** - Use Read tool on relevant files
3. **Check test files** - Tests show how code is meant to be used
4. **Review git history** - `git log --oneline --graph` shows evolution
5. **Check documentation** - See docs/ directory

### For Humans

- GitHub Issues: Report bugs or request features
- Documentation: See docs/ directory
- Tests: Run `pytest tests/ -v` for examples

---

## üìä Quality Standards

This project maintains **world-class quality**:

- ‚úÖ 100% functional test pass rate
- ‚úÖ Type safety with mypy strict mode
- ‚úÖ Clean Architecture principles
- ‚úÖ Comprehensive error handling
- ‚úÖ Structured logging and observability
- ‚úÖ Enterprise-grade resilience patterns
- ‚úÖ Security best practices

**When making changes, maintain these standards!**

---

## üéì Learning Resources

### Understanding the Codebase

1. **Start with README.md** - High-level overview
2. **Read ARCHITECTURE.md** - System design
3. **Explore CODEBASE_MAP.md** - Navigate structure
4. **Check examples/** - See usage patterns
5. **Read tests/** - Understand expected behavior

### Key Concepts

- **Clean Architecture**: Separation of concerns, dependency inversion
- **Domain-Driven Design**: Core domain logic in orchestrator/core/
- **Observability**: Metrics, logging, monitoring
- **Resilience**: Circuit breaker, retry, bulkhead patterns
- **Type Safety**: Comprehensive type hints with mypy

---

## üéâ Success Criteria

You're successfully working with this codebase when:

- ‚úÖ All tests pass after your changes
- ‚úÖ No mypy type errors in modified code
- ‚úÖ Changes follow existing patterns
- ‚úÖ Code is properly documented
- ‚úÖ Commits are focused and clear
- ‚úÖ Quality standards are maintained

---

**Last Updated:** 2025-10-21
**Version:** v10.1.0
**Status:** Production Ready - World-class Quality ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
